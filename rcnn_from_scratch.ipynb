{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-sokF8rqQEV",
        "outputId": "ccb97978-951f-4e31-aa9d-9c2f2b48bba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "1OZywm3EqiZb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc_dataset_train = torchvision.datasets.VOCDetection(root=\"content/voc\",\n",
        "                                                image_set=\"train\",\n",
        "                                                download=True,\n",
        "                                                year=\"2007\")\n",
        "voc_dataset_val = torchvision.datasets.VOCDetection(root=\"content/voc\",\n",
        "                                                image_set=\"val\",\n",
        "                                                download=True,\n",
        "                                                year=\"2007\")"
      ],
      "metadata": {
        "id": "ioNBt1AFskns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc_dataset_train[0]"
      ],
      "metadata": {
        "id": "1pUwCyyMqiVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample Image shape: \", np.array(voc_dataset_train[0][0]).shape)"
      ],
      "metadata": {
        "id": "tWujlGtqqiTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_objs = []\n",
        "for ds in voc_dataset_train:\n",
        "    obj_annots = ds[1][\"annotation\"][\"object\"]\n",
        "    for obj in obj_annots:\n",
        "        all_objs.append(obj[\"name\"])\n",
        "\n",
        "unique_class_labels = set(all_objs)\n",
        "print(\"Number of unique objects in dataset: \", len(unique_class_labels))\n",
        "print(\"Unique labels in dataset: \\n\", unique_class_labels)"
      ],
      "metadata": {
        "id": "xqBqPNTLqiRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_2_idx = {'pottedplant': 1, 'person': 2,\n",
        "               'motorbike': 3, 'train': 4,\n",
        "               'dog': 5, 'diningtable': 6,\n",
        "               'horse': 7, 'bus': 8,\n",
        "               'aeroplane': 9, 'sofa': 10,\n",
        "               'sheep': 11, 'tvmonitor': 12,\n",
        "               'bird': 13, 'bottle': 14,\n",
        "               'chair': 15, 'cat': 16,\n",
        "               'bicycle': 17, 'cow': 18,\n",
        "               'boat': 19, 'car': 20, 'bg': 0}\n",
        "idx_2_label = {1: 'pottedplant', 2: 'person',\n",
        "               3: 'motorbike', 4: 'train',\n",
        "               5: 'dog', 6: 'diningtable',\n",
        "               7: 'horse', 8: 'bus',\n",
        "               9: 'aeroplane', 10: 'sofa',\n",
        "               11: 'sheep', 12: 'tvmonitor',\n",
        "               13: 'bird', 14: 'bottle',\n",
        "               15: 'chair', 16: 'cat',\n",
        "               17: 'bicycle', 18: 'cow',\n",
        "               19: 'boat', 20: 'car', 0: 'bg'}"
      ],
      "metadata": {
        "id": "LK1VjgvHqvq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_boxes(img, boxes, scores, labels, class_map=None):\n",
        "    nums = len(boxes)\n",
        "    for i in range(nums):\n",
        "        x1y1 = tuple((np.array(boxes[i][0:2])).astype(np.int32))\n",
        "        x2y2 = tuple((np.array(boxes[i][2:4])).astype(np.int32))\n",
        "        img = cv2.rectangle(img, x1y1, x2y2, (255, 0, 0), 2)\n",
        "        label = int(labels[i])\n",
        "        if class_map is not None:\n",
        "            label_txt = class_map[label]\n",
        "        else:\n",
        "            label_txt = str(label)\n",
        "        img = cv2.putText(\n",
        "            img,\n",
        "            \"{} {:.4f}\".format(label_txt, scores[i]),\n",
        "            x1y1,\n",
        "            cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
        "            1,\n",
        "            (0, 0, 255),\n",
        "            2,\n",
        "        )\n",
        "    return img"
      ],
      "metadata": {
        "id": "VKyNdnqZqiPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image, sample_annot = voc_dataset_train[0]\n",
        "sample_image = np.array(sample_image)\n",
        "sample_annot = sample_annot[\"annotation\"][\"object\"]\n",
        "boxes = [[int(v) for k, v in x[\"bndbox\"].items()] for x in sample_annot]\n",
        "labels = [label_2_idx[x[\"name\"]] for x in sample_annot]\n",
        "scores = [1]*len(labels)\n",
        "final_image = draw_boxes(sample_image, boxes, scores, labels, idx_2_label)\n",
        "plt.imshow(final_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uKdTujeGqiNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou_score(box_1, box_2):\n",
        "\n",
        "    box_1_x1 = box_1[0]\n",
        "    box_1_y1 = box_1[1]\n",
        "    box_1_x2 = box_1[2]\n",
        "    box_1_y2 = box_1[3]\n",
        "\n",
        "    box_2_x1 = box_2[0]\n",
        "    box_2_y1 = box_2[1]\n",
        "    box_2_x2 = box_2[2]\n",
        "    box_2_y2 = box_2[3]\n",
        "\n",
        "    x1 = np.maximum(box_1_x1, box_2_x1)\n",
        "    y1 = np.maximum(box_1_y1, box_2_y1)\n",
        "    x2 = np.minimum(box_1_x2, box_2_x2)\n",
        "    y2 = np.minimum(box_1_y2, box_2_y2)\n",
        "\n",
        "    area_of_intersection = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
        "    area_box_1 = (box_1_x2 - box_1_x1 + 1) * (box_1_y2 - box_1_y1 + 1)\n",
        "    area_box_2 = (box_2_x2 - box_2_x1 + 1) * (box_2_y2 - box_2_y1 + 1)\n",
        "    area_of_union = area_box_1 + area_box_2 - area_of_intersection\n",
        "\n",
        "    return area_of_intersection/float(area_of_union)"
      ],
      "metadata": {
        "id": "GK3R6IXpqiLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_for_rcnn(image, rects, class_map, boxes_annots, iou_threshold, max_boxes):\n",
        "    true_classes = []\n",
        "    image_sections = []\n",
        "    true_count = 0\n",
        "    false_count = 0\n",
        "    for annot in boxes_annots:\n",
        "        label = annot[\"name\"]\n",
        "        box = [int(c) for _, c in annot[\"bndbox\"].items()]\n",
        "        box = np.array(box)\n",
        "        for rect in rects:\n",
        "            iou_score = calculate_iou_score(rect, box)\n",
        "            if iou_score > iou_threshold:\n",
        "                if true_count < max_boxes//2:\n",
        "                    true_classes.append(class_map[label])\n",
        "                    x1, y1, x2, y2 = rect\n",
        "                    img_section = image[y1: y2, x1: x2]\n",
        "                    image_sections.append(img_section)\n",
        "                    true_count += 1\n",
        "            else:\n",
        "                if false_count < max_boxes//2:\n",
        "                    true_classes.append(0)\n",
        "                    x1, y1, x2, y2 = rect\n",
        "                    img_section = image[y1: y2, x1: x2]\n",
        "                    image_sections.append(img_section)\n",
        "                    false_count += 1\n",
        "    return image_sections, true_classes"
      ],
      "metadata": {
        "id": "tk2QJm9cqiJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iou_threshold = 0.7\n",
        "max_boxes = 50\n",
        "max_selections = 1000\n",
        "processed_data_save_path_train = \"/kaggle/input/rcnn-processed-pickle/rcnn_train/rcnn_train\"\n",
        "processed_data_save_path_val = \"/kaggle/input/rcnn-processed-pickle/rcnn_val/rcnn_val\"\n",
        "os.makedirs(processed_data_save_path_train, exist_ok=True)\n",
        "os.makedirs(processed_data_save_path_val, exist_ok=True)"
      ],
      "metadata": {
        "id": "fIWuLadzqiHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_images = []\n",
        "all_labels = []\n",
        "count = 0\n",
        "if len(os.listdir(processed_data_save_path_train)) < 80000:\n",
        "    for image, annot in tqdm(voc_dataset_train):\n",
        "        image = np.array(image)\n",
        "        boxes_annots = annot[\"annotation\"][\"object\"]\n",
        "        ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
        "        ss.setBaseImage(image)\n",
        "        ss.switchToSelectiveSearchFast()\n",
        "        rects = ss.process()[:max_selections]\n",
        "        rects = np.array([[x, y, x+w, y+h] for x, y, w, h in rects])\n",
        "        images, classes = process_data_for_rcnn(image,\n",
        "                                                rects,\n",
        "                                                label_2_idx,\n",
        "                                                boxes_annots,\n",
        "                                                max_iou_threshold,\n",
        "                                                max_boxes)\n",
        "        count += 1\n",
        "        all_images += images\n",
        "        all_labels += classes\n",
        "\n",
        "    # saving processed data to pickle file\n",
        "    for idx, (image, label) in enumerate(zip(all_images, all_labels)):\n",
        "        with open(os.path.join(processed_data_save_path_train, f\"img_{idx}.pkl\"), \"wb\") as pkl:\n",
        "            pickle.dump({\"image\": image, \"label\": label}, pkl)\n",
        "else:\n",
        "    print(\"Data Already Prepared.\")"
      ],
      "metadata": {
        "id": "DgUOFS5trBFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RCNNDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, processed_data_folder, section_dim=(224, 224)):\n",
        "        self.section_dim = section_dim\n",
        "        self.data_files = os.listdir(processed_data_folder)\n",
        "        self.data_files = list(map(lambda x: os.path.join(processed_data_folder, x), self.data_files))\n",
        "        self.preprocess = torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        with open(self.data_files[idx], \"rb\") as pkl:\n",
        "            data = pickle.load(pkl)\n",
        "        image, label = data[\"image\"], data[\"label\"]\n",
        "        image = cv2.resize(image, self.section_dim)\n",
        "        image = np.asarray(image, dtype=np.float32)\n",
        "        image = torch.from_numpy(image)\n",
        "        image = torch.permute(image, (2, 0, 1))\n",
        "        image = self.preprocess(image)\n",
        "        label = torch.tensor(label)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "uz6JwynNqiFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(inp, labels, num_rows=16, num_cols=4):\n",
        "    \"\"\"Display image for Tensor.\"\"\"\n",
        "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 30))\n",
        "    axes = axes.ravel()\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).reshape(1, -1, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).reshape(1, -1, 1, 1)\n",
        "    inp = std * inp + mean\n",
        "    inp = inp.permute((0, 2, 3, 1))\n",
        "    inp = inp.type(torch.uint8)\n",
        "    for idx, ax in enumerate(axes):\n",
        "        ax.imshow(inp[idx])\n",
        "        ax.set_title(labels[idx])\n",
        "        ax.grid(False)\n",
        "        ax.set_axis_off()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "uOz4jdunqiCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = RCNNDataset(processed_data_folder=processed_data_save_path_train, section_dim=(224, 224))\n",
        "val_dataset = RCNNDataset(processed_data_folder=processed_data_save_path_val, section_dim=(224, 224))"
      ],
      "metadata": {
        "id": "aS3j1P8Yqh9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Dataset one sample images shape: \", train_dataset[0][0].shape)\n",
        "print(\"Train Dataset one sample labels shape: \", train_dataset[0][1].shape)\n",
        "print(\"Train Dataset one sample images dtype: \", train_dataset[0][0].dtype)\n",
        "print(\"Train Dataset one sample labels dtype: \", train_dataset[0][1].dtype)\n",
        "print(\"Train Dataset number of samples: \", len(train_dataset))"
      ],
      "metadata": {
        "id": "KrHRcVHwqhy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Val Dataset one sample images shape: \", val_dataset[0][0].shape)\n",
        "print(\"Val Dataset one sample labels shape: \", val_dataset[0][1].shape)\n",
        "print(\"Val Dataset one sample images dtype: \", val_dataset[0][0].dtype)\n",
        "print(\"Val Dataset one sample labels dtype: \", val_dataset[0][1].dtype)\n",
        "print(\"Val Dataset number of samples: \", len(val_dataset))"
      ],
      "metadata": {
        "id": "N0URuL5Bqhwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "nEMkaj4LqhuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs, labels = next(iter(train_loader))\n",
        "labels = [idx_2_label[x.item()] for x in labels]\n",
        "print(\"Train Batch\")\n",
        "imshow(inputs, labels, num_rows=8, num_cols=4)"
      ],
      "metadata": {
        "id": "kwQe_KoGqhsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, labels = next(iter(val_loader))\n",
        "labels = [idx_2_label[x.item()] for x in labels]\n",
        "print(\"Validation Batch\")\n",
        "imshow(inputs, labels, num_rows=8, num_cols=4)"
      ],
      "metadata": {
        "id": "fAgGJpF7qhos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using Device: \", device)"
      ],
      "metadata": {
        "id": "TK3WFI1RqhmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(backbone, num_classes):\n",
        "    num_ftrs = backbone.fc.in_features\n",
        "    # num_classes = number of class categories and +1 for background class\n",
        "    backbone.fc = nn.Sequential(nn.Dropout(0.2),\n",
        "                                nn.Linear(num_ftrs, 512),\n",
        "                                nn.Dropout(0.2),\n",
        "                                nn.Linear(512, num_classes+1))\n",
        "    return backbone"
      ],
      "metadata": {
        "id": "lET2-3N2qhkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_backbone = torchvision.models.resnet50(weights='IMAGENET1K_V2')\n",
        "# freeze pretrained backbone\n",
        "for param in resnet_backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "model = build_model(backbone=resnet_backbone, num_classes=len(unique_class_labels))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "PsjGpMpoqhie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = [1.0]+[2.0]*len(unique_class_labels) # 1 for bg and 2 for other classes\n",
        "class_weights = torch.tensor(class_weights).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "K41g8B3-qhgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "num_epochs = 100\n",
        "best_val_loss = 1000\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "count = 0\n",
        "for idx in range(num_epochs):\n",
        "    train_losses = []\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "    model.train()\n",
        "    for images, labels in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        pred = model(images)\n",
        "        loss = criterion(pred, labels)\n",
        "        predicted = torch.argmax(pred, 1)\n",
        "        total_train += labels.shape[0]\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "    accuracy_train = (100 * correct_train) / total_train\n",
        "    train_accuracy.append(accuracy_train)\n",
        "    epoch_train_loss = np.mean(train_losses)\n",
        "    epoch_train_losses.append(epoch_train_loss)\n",
        "\n",
        "    val_losses = []\n",
        "    total_val = 0\n",
        "    correct_val = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            pred = model(images)\n",
        "            loss = criterion(pred, labels)\n",
        "            val_losses.append(loss.item())\n",
        "            predicted = torch.argmax(pred, 1)\n",
        "            total_val += labels.shape[0]\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy_val = (100 * correct_val) / total_val\n",
        "    val_accuracy.append(accuracy_val)\n",
        "    epoch_val_loss = np.mean(val_losses)\n",
        "    epoch_val_losses.append(epoch_val_loss)\n",
        "\n",
        "    print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Train Accuracy: {:.8f}, Val Loss: {:.8f}, Val Accuracy: {:.8f}'.format(idx + 1, num_epochs, epoch_train_loss, accuracy_train, epoch_val_loss, accuracy_val))\n",
        "\n",
        "\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        print(\"Saving the model state dictionary for Epoch: {} with Validation loss: {:.8f}\".format(idx + 1, epoch_val_loss))\n",
        "        torch.save(model.state_dict(), \"rcnn_model.pt\")\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "\n",
        "    if count == 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "cy3swhfVqhdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_transform = torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                        [0.229, 0.224, 0.225])\n",
        "def process_inputs(image, max_selections=300, section_size=(224, 224)):\n",
        "    images = []\n",
        "    boxes = []\n",
        "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
        "    ss.setBaseImage(image)\n",
        "    ss.switchToSelectiveSearchQuality()\n",
        "    rects = ss.process()[:max_selections]\n",
        "    rects = np.array([[x, y, x+w, y+h] for x, y, w, h in rects])\n",
        "    for rect in rects:\n",
        "        x1, y1, x2, y2 = rect\n",
        "        img_section = image[y1: y2, x1: x2]\n",
        "        img_section = cv2.resize(img_section, section_size)\n",
        "        images.append(img_section)\n",
        "        boxes.append(rect)\n",
        "    images = np.array(images, dtype=np.float32)\n",
        "    images = torch.from_numpy(images)\n",
        "    images = images.permute(0, 3, 1, 2)\n",
        "    images = normalized_transform(images)\n",
        "    return images, np.array(boxes)"
      ],
      "metadata": {
        "id": "TVjiId7FqhaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def non_max_supression(boxes, scores, labels, threshold=0.5, iou_threshold=0.5):\n",
        "    idxs = np.where(scores>threshold)\n",
        "    boxes = boxes[idxs]\n",
        "    scores = scores[idxs]\n",
        "    labels = labels[idxs]\n",
        "    idxs = np.argsort(scores)\n",
        "    chossen_boxes = []\n",
        "    choosen_boxes_scores = []\n",
        "    choosen_boxes_labels = []\n",
        "    while len(idxs):\n",
        "        last = len(idxs) - 1\n",
        "        choosen_idx = idxs[last]\n",
        "        choosen_box = boxes[choosen_idx]\n",
        "        choosen_box_score = scores[choosen_idx]\n",
        "        choosen_box_label = labels[choosen_idx]\n",
        "        chossen_boxes.append(choosen_box)\n",
        "        choosen_boxes_scores.append(choosen_box_score)\n",
        "        choosen_boxes_labels.append(choosen_box_label)\n",
        "        idxs = np.delete(idxs, last)\n",
        "        i = len(idxs)-1\n",
        "        while i >= 0:\n",
        "            idx = idxs[i]\n",
        "            curr_box = boxes[idx]\n",
        "            curr_box_score = scores[idx]\n",
        "            curr_box_label = labels[idx]\n",
        "            if (curr_box_label == choosen_box_label and\n",
        "                calculate_iou_score(curr_box, choosen_box) > iou_threshold):\n",
        "                idxs = np.delete(idxs, i)\n",
        "            i -= 1\n",
        "    return chossen_boxes, choosen_boxes_scores, choosen_boxes_labels"
      ],
      "metadata": {
        "id": "QBJCtAG2s3K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_outputs(scores, boxes, threshold=0.5, iou_threshold=0.5):\n",
        "    labels = np.argmax(scores, axis=1)\n",
        "    probas = np.max(scores, axis=1)\n",
        "    idxs = labels != 0\n",
        "    boxes = boxes[idxs]\n",
        "    probas = probas[idxs]\n",
        "    labels = labels[idxs]\n",
        "    assert len(probas) == len(boxes) == len(labels)\n",
        "    final_boxes, final_boxes_scores, final_boxes_labels = non_max_supression(boxes, probas, labels, threshold, iou_threshold)\n",
        "    return final_boxes, final_boxes_scores, final_boxes_labels"
      ],
      "metadata": {
        "id": "hR9it5bDs3Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading best model\n",
        "model.load_state_dict(torch.load(\"rcnn_model.pt\"))"
      ],
      "metadata": {
        "id": "Qdo9i-pxs3Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_image = np.array(voc_dataset_val[0][0])\n",
        "# preprocess input image\n",
        "prep_val_images, prep_val_boxes = process_inputs(val_image)\n",
        "prep_val_images.shape, prep_val_images.dtype, prep_val_boxes.shape, prep_val_boxes.dtype"
      ],
      "metadata": {
        "id": "OBKgDc6Gs3Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(prep_val_images.to(device))\n",
        "# postprocess output from model\n",
        "scores = torch.softmax(output, dim=1).cpu().numpy()\n",
        "boxes, boxes_scores, boxes_labels = process_outputs(scores, prep_val_boxes, threshold=0.5, iou_threshold=0.5)"
      ],
      "metadata": {
        "id": "iK9gXfQXs2-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_image = draw_boxes(val_image,\n",
        "                         boxes,\n",
        "                         boxes_scores,\n",
        "                         boxes_labels,\n",
        "                         idx_2_label)\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(final_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6h1VyCkts28U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(image, only_boxed_image=False, label_map=None, max_boxes=100, threshold=0.5, iou_threshold=0.5):\n",
        "    # preprocess input image\n",
        "    prep_val_images, prep_val_boxes = process_inputs(image, max_selections=max_boxes)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(prep_val_images.to(device))\n",
        "    # postprocess output from model\n",
        "    scores = torch.softmax(output, dim=1).cpu().numpy()\n",
        "    boxes, boxes_scores, boxes_labels = process_outputs(scores,\n",
        "                                                        prep_val_boxes,\n",
        "                                                        threshold=threshold,\n",
        "                                                        iou_threshold=iou_threshold)\n",
        "    if only_boxed_image:\n",
        "        box_image = draw_boxes(image, boxes, boxes_scores, boxes_labels, label_map)\n",
        "        return box_image\n",
        "    return boxes, boxes_scores, boxes_labels"
      ],
      "metadata": {
        "id": "bnMn0sA8s24a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    image = np.array(voc_dataset_val[i][0])\n",
        "    final_image = predict(image, only_boxed_image=True,\n",
        "                          label_map=idx_2_label,\n",
        "                          threshold=0.5, iou_threshold=0.5)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(final_image)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "goaiWDkDs22V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZR2omD5s20h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CcZ4b_Y7s2xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lnLCcPioqhYZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}